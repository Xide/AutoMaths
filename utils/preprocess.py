"""
Data pre-processing module.

This module is intented to preprocess raw COQ .v files into CSV ones.
"""

import os
import re
import sys
import json
import argparse
import logging

from pygments import highlight
from pygments.lexers.theorem import CoqLexer
from pygments.formatters import RawTokenFormatter

log = None


def generate_parser():
    """Return the argv parser for pre-processing."""
    parser = argparse.ArgumentParser()
    # parser.add_argument(
    #     'data_directory',
    #     help='Directory where data was downloaded'
    # )
    # parser.add_argument(
    #     '-r',
    #     '--recursive',
    #     action='store_true',
    #     help='Search for files in subdirectories of `data_directory`'
    # )
    parser.add_argument(
        '-v',
        '--verbose',
        action='count',
        help='How verbose the application should be. -vvv for max'
    )
    parser.add_argument(
        '-o',
        '--output',
        type=str,
        help='Destination of the processed file',
        default='out.om'
    )

    parser.add_argument(
        'source_file',
        type=str,
        help='Source file to process'
    )
    return parser


def setup_logger(verbose: int):
    """
    Configure the python logger.

    :params verbose: `int` between 0 and 3.
    """
    if verbose > 3:
        verbose = 3
    verbose_levels = [
        logging.ERROR,
        logging.WARNING,
        logging.INFO,
        logging.DEBUG
    ]
    global log
    log = logging.getLogger('preprocess')
    log.setLevel(verbose_levels[verbose])
    try:
        import coloredlogs
        coloredlogs.install(
            level=verbose_levels[verbose],
            logger=log
        )
    except ImportError:
        pass


class ProofLogger:
    """Filter parts of COQ tokens that are proofs."""

    def __init__(self, log_fn=None):
        """
        Initialize the filer.

        :params fuction used to log
        :type function(str)
        """
        self.is_in_proof = False
        self.log_fn = log_fn

    def feed(self, token, raw):
        """Feed logger with raw datas."""
        # Debug: Print every proof found in the token stream
        if not self.log_fn:
            return
        if token == 'Keyword.Namespace' and 'Proof' in raw:
            self.is_in_proof = True
            self.log_fn('>>>')
        if self.is_in_proof:
            self.log_fn('token: "{}", raw: "{}"'.format(token, raw))
        if token == 'Keyword.Namespace' and \
                ('Qed' in raw or 'Defined' in raw):
            self.is_in_proof = False
            self.log_fn('<<<')


def check_raw_token_syntax(content_lines, regex):
    """Check if lines match `regex` format."""
    return list(
        filter(
            lambda x: x is not None,
            map(
                lambda x:
                    None
                    if regex.match(x[1])
                    else x,
                enumerate(content_lines)
            )
        )
    )


if __name__ == '__main__':
    # Parse command line arguments
    args = generate_parser().parse_args()

    script_dir = os.path.dirname(os.path.realpath(__file__))
    if os.path.isabs(args.source_file):
        source_file = args.source_file
    else:
        source_file = os.path.realpath(
            os.path.join(os.getcwd(), args.source_file)
        )

    if not args.verbose:
        args.verbose = 0
    setup_logger(args.verbose)

    # for fname in \
    #         find_files(
    #             data_dir,
    #             regex='^.*\.v$',
    #             recursive=args.recursive
    #         ):
    log.debug('[PARSING] {}'.format(source_file))
    with open(source_file, 'r') as f:
        file_content = f.read()

    # `Pygments` lexing.
    lexed_content = highlight(
        file_content,
        CoqLexer(),
        RawTokenFormatter()
    )

    # Load the entire file contents into RAM as string
    # IMPROVMENT: Enhance the `RawTokenFormatter` class to stream this data
    parsed_content = str(
        lexed_content,
        encoding='utf-8'
    ).splitlines()
    del lexed_content

    # Regular expression matching a raw token line.
    regex = re.compile('Token\.((?:\w+\.?)+)\s(.*)\n?')

    # Detect errors in file syntax
    parsing_errors = check_raw_token_syntax(parsed_content, regex)

    # If the line format does not fit with regex, log an error
    # and exit the application
    if len(parsing_errors):
        log.error(
            'Syntax error:\n- {}'.format(
                '\n -'.join(
                    map(
                        lambda l, x: 'line {} : {}'.format(
                            l, x
                        ),
                        parsing_errors
                    )
                )
            )
        )
        sys.exit(1)

    # Write the token stream generated by pygments into text file.
    # change extension of exported file from `.v` to `.om`
    with open(args.output, 'w') as f:
        json.dump({
            'contents': list(map(
                lambda x: (x.group(1), x.group(2)),
                map(
                    lambda x: regex.match(x),
                    parsed_content
                )
            ))
        }, f)

        # proof_logger = ProofLogger()
        # with open(
        #         '{}.ans'.format(
        #             '.'.join(fname.split('.')[:-1])
        #             ),
        #         'w'
        #         ) as f:
        #     # Iterate over every token
        #     for line in parsed_tab:
        #         match = regex.match(line)
        #
        #         if match is None:
        #             log.error(
        #                 'Line "{}" does not fit with the regex.'.format(line)
        #             )
        #             sys.exit(1)
        #
        #         # Extract groups from the regex
        #         token, raw = match.group(1), match.group(2)
        #
        #         # Log them if they are part of a proof
        #         proof_logger.feed(token, raw)
        #
        #         # Write the token into file for analysis
        #         f.write('{}\n'.format(token))
