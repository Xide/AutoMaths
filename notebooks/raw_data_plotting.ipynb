{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Install package dependencies\n",
    "\n",
    "! pip install --user pandas xarray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the dataset into a pandas DataFrame\n",
    "\n",
    "DATASET_PATH=os.path.join(os.path.dirname(os.getcwd()), 'results.csv')\n",
    "print('Loading dataset from \"{}\".'.format(DATASET_PATH))\n",
    "df = pd.DataFrame.from_csv(DATASET_PATH)\n",
    "print('Done.')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def token_barplot(df, **kwargs):\n",
    "    \"\"\"Plot the repartition of tokens in the dataset.\"\"\"\n",
    "\n",
    "    tokens = df['token']\n",
    "    tokens_counts = pd.DataFrame.from_dict({\n",
    "            x: len(df[df['token'] == x]) for x in tokens.unique()\n",
    "        },\n",
    "        orient='index'\n",
    "    )\n",
    "\n",
    "#     tokens_counts[1] = [100 * (x / sum(tokens_counts[0])) for x in tokens_counts[0]]\n",
    "    return tokens_counts.plot.barh(**kwargs)\n",
    "\n",
    "def token_pieplot(df, **kwargs):\n",
    "    \"\"\"Plot the repartition of tokens in the dataset.\"\"\"\n",
    "\n",
    "    tokens = df['token']\n",
    "    tokens_counts = pd.DataFrame.from_dict({\n",
    "            x: len(df[df['token'] == x]) for x in tokens.unique()\n",
    "        },\n",
    "        orient='index'\n",
    "    )\n",
    "\n",
    "#     tokens_counts[1] = [100 * (x / sum(tokens_counts[0])) for x in tokens_counts[0]]\n",
    "    return tokens_counts.plot.pie(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "token_barplot(df, title='Raw token repartition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dropping errors from the dataset\n",
    "# TODO: Fix them instead, some file are lost in the process\n",
    "n = len(df)\n",
    "print('Preparing to clean data, dataset length :', n)\n",
    "\n",
    "df = df[df['token'] != 'Error']\n",
    "print('Number of errors tokens dropped      :', n - len(df))\n",
    "\n",
    "# Dropping comments, unneeded in the learning process\n",
    "n = len(df)\n",
    "df = df[df['token'] != 'Comment']\n",
    "print('Number of comment tokents dropped    :', n - len(df))\n",
    "\n",
    "import re\n",
    "\n",
    "# Dropping empty strings of text tokens as they are irrelevent for the learning process\n",
    "# since the input is already splitted by the lexer.\n",
    "\n",
    "n = len(df)\n",
    "empty_string_regex = re.compile('^\\s*$')\n",
    "not_empty_idxs = list(\n",
    "    map(\n",
    "        lambda x: empty_string_regex.match(x) is None,\n",
    "        df['raw']\n",
    "    )\n",
    ")\n",
    "\n",
    "df = df[not_empty_idxs]\n",
    "print('Number of empty text tokens dropped  :', n - len(df))\n",
    "\n",
    "\n",
    "n = len(df)\n",
    "print('Data cleaned, dataset length :', n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "token_barplot(df, title='Clean token repartition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "in_proofs = df[df['proof_id'].notnull()]\n",
    "\n",
    "print('Number of proofs          :', len(in_proofs['proof_id'].unique()))\n",
    "\n",
    "print('Number of lines of proofs :', len(in_proofs))\n",
    "\n",
    "print('Number of lines           :', len(df))\n",
    "\n",
    "token_barplot(in_proofs, title='Token repartition in a proof')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_proof(df, uid):\n",
    "    return df[df['proof_id'] == uid]\n",
    "\n",
    "proofs = xr.DataArray(\n",
    "    dims=(\n",
    "        'file_id',\n",
    "        'proof_id',\n",
    "        'token_id',\n",
    "        'token',\n",
    "        'raw'\n",
    "    )\n",
    ")\n",
    "\n",
    "for uid in df['proof_id'].unique():\n",
    "    print(uid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
